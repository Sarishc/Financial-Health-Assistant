{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/time_series_analysis.ipynb\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Time Series Analysis of Financial Transactions\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates the time series preprocessing and analysis capabilities of the Financial Health Assistant.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add parent directory to path for imports\\n\",\n",
    "    \"sys.path.append(os.path.dirname(os.getcwd()))\\n\",\n",
    "    \"\\n\",\n",
    "    \"from app.models.time_series.time_series_processor import TimeSeriesProcessor\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set plot style\\n\",\n",
    "    \"plt.style.use('ggplot')\\n\",\n",
    "    \"sns.set(font_scale=1.2)\\n\",\n",
    "    \"%matplotlib inline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Transaction Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load transaction data\\n\",\n",
    "    \"transactions_path = '../app/data/processed/transactions_clean.csv'\\n\",\n",
    "    \"\\n\",\n",
    "    \"if os.path.exists(transactions_path):\\n\",\n",
    "    \"    df = pd.read_csv(transactions_path)\\n\",\n",
    "    \"    print(f\\\"Loaded {len(df)} transactions\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Error: {transactions_path} not found\\\")\\n\",\n",
    "    \"    # Create sample data for demonstration if necessary\\n\",\n",
    "    \"    from datetime import datetime, timedelta\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Generate dates\\n\",\n",
    "    \"    base_date = datetime(2023, 1, 1)\\n\",\n",
    "    \"    dates = [base_date + timedelta(days=i) for i in range(100)]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Generate random transactions\\n\",\n",
    "    \"    np.random.seed(42)\\n\",\n",
    "    \"    n_transactions = 300\\n\",\n",
    "    \"    transaction_dates = np.random.choice(dates, size=n_transactions)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Generate random categories\\n\",\n",
    "    \"    categories = ['food', 'transport', 'shopping', 'utilities', 'entertainment']\\n\",\n",
    "    \"    transaction_categories = np.random.choice(categories, size=n_transactions)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Generate random amounts (negative for expenses)\\n\",\n",
    "    \"    amounts = -np.random.uniform(10, 200, size=n_transactions)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add some income transactions (positive amounts)\\n\",\n",
    "    \"    income_indices = np.random.choice(range(n_transactions), size=30)\\n\",\n",
    "    \"    amounts[income_indices] = np.random.uniform(500, 2000, size=30)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create DataFrame\\n\",\n",
    "    \"    df = pd.DataFrame({\\n\",\n",
    "    \"        'transaction_date': transaction_dates,\\n\",\n",
    "    \"        'category': transaction_categories,\\n\",\n",
    "    \"        'amount': amounts\\n\",\n",
    "    \"    })\\n\",\n",
    "    \"    print(f\\\"Created sample data with {len(df)} transactions\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display sample\\n\",\n",
    "    \"df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Preprocess Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Map columns if needed\\n\",\n",
    "    \"column_mapping = {\\n\",\n",
    "    \"    'DATE': 'transaction_date',\\n\",\n",
    "    \"    ' WITHDRAWAL AMT ': 'withdrawal',\\n\",\n",
    "    \"    ' DEPOSIT AMT ': 'deposit',\\n\",\n",
    "    \"    'TRANSACTION DETAILS': 'description'\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"for old_col, new_col in column_mapping.items():\\n\",\n",
    "    \"    if old_col in df.columns and new_col not in df.columns:\\n\",\n",
    "    \"        df[new_col] = df[old_col]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Ensure date column is datetime\\n\",\n",
    "    \"if 'transaction_date' in df.columns:\\n\",\n",
    "    \"    df['transaction_date'] = pd.to_datetime(df['transaction_date'], errors='coerce')\\n\",\n",
    "    \"    df = df.dropna(subset=['transaction_date'])\\n\",\n",
    "    \"    print(f\\\"Date range: {df['transaction_date'].min()} to {df['transaction_date'].max()}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create amount column if it doesn't exist\\n\",\n",
    "    \"if 'amount' not in df.columns and 'withdrawal' in df.columns and 'deposit' in df.columns:\\n\",\n",
    "    \"    df['withdrawal'] = pd.to_numeric(df['withdrawal'], errors='coerce').fillna(0)\\n\",\n",
    "    \"    df['deposit'] = pd.to_numeric(df['deposit'], errors='coerce').fillna(0)\\n\",\n",
    "    \"    df['amount'] = df['deposit'] - df['withdrawal']\\n\",\n",
    "    \"    print(\\\"Created 'amount' column from withdrawal and deposit columns\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize time series processor\\n\",\n",
    "    \"processor = TimeSeriesProcessor()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Aggregate Transactions to Time Series Format\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create different frequency time series\\n\",\n",
    "    \"daily_ts = processor.convert_to_time_series(\\n\",\n",
    "    \"    df,\\n\",\n",
    "    \"    date_col='transaction_date',\\n\",\n",
    "    \"    amount_col='amount',\\n\",\n",
    "    \"    category_col='category',\\n\",\n",
    "    \"    freq='D'\\n\",\n",
    "    \")\\n\",\n",
    "    \"print(f\\\"Created daily time series with {len(daily_ts)} days\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"weekly_ts = processor.convert_to_time_series(\\n\",\n",
    "    \"    df,\\n\",\n",
    "    \"    date_col='transaction_date',\\n\",\n",
    "    \"    amount_col='amount',\\n\",\n",
    "    \"    category_col='category',\\n\",\n",
    "    \"    freq='W'\\n\",\n",
    "    \")\\n\",\n",
    "    \"print(f\\\"Created weekly time series with {len(weekly_ts)} weeks\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"monthly_ts = processor.convert_to_time_series(\\n\",\n",
    "    \"    df,\\n\",\n",
    "    \"    date_col='transaction_date',\\n\",\n",
    "    \"    amount_col='amount',\\n\",\n",
    "    \"    category_col='category',\\n\",\n",
    "    \"    freq='M'\\n\",\n",
    "    \")\\n\",\n",
    "    \"print(f\\\"Created monthly time series with {len(monthly_ts)} months\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display monthly time series\\n\",\n",
    "    \"monthly_ts.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Visualize Time Series Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot monthly spending\\n\",\n",
    "    \"plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"if 'category' in monthly_ts.columns:\\n\",\n",
    "    \"    # Create pivot table for better visualization\\n\",\n",
    "    \"    pivot_df = monthly_ts.pivot(index='transaction_date', columns='category', values='amount_sum')\\n\",\n",
    "    \"    pivot_df.plot(kind='bar', stacked=True, ax=plt.gca())\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    monthly_ts.plot(x='transaction_date', y='amount_sum', kind='bar', ax=plt.gca())\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.title('Monthly Total Spending')\\n\",\n",
    "    \"plt.xlabel('Month')\\n\",\n",
    "    \"plt.ylabel('Amount')\\n\",\n",
    "    \"plt.xticks(rotation=45)\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Extract Temporal Features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Add temporal features to daily time series\\n\",\n",
    "    \"daily_features = processor.extract_temporal_features(daily_ts)\\n\",\n",
    "    \"print(f\\\"Added {len(daily_features.columns) - len(daily_ts.columns)} temporal features\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display new columns\\n\",\n",
    "    \"new_cols = [col for col in daily_features.columns if col not in daily_ts.columns]\\n\",\n",
    "    \"print(\\\"New features:\\\", new_cols)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display sample of data with features\\n\",\n",
    "    \"daily_features[['transaction_date', 'amount_sum'] + new_cols].head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Analyze Day of Week Patterns\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze spending by day of week\\n\",\n",
    "    \"if 'dayofweek' in daily_features.columns:\\n\",\n",
    "    \"    plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create day of week labels\\n\",\n",
    "    \"    day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if 'category' in daily_features.columns:\\n\",\n",
    "    \"        # Group by day of week and category\\n\",\n",
    "    \"        dow_spending = daily_features.groupby(['dayofweek', 'category'])['amount_sum'].mean().reset_index()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create pivot table\\n\",\n",
    "    \"        pivot_df = dow_spending.pivot(index='dayofweek', columns='category', values='amount_sum')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot stacked bar chart\\n\",\n",
    "    \"        pivot_df.plot(kind='bar', stacked=True, ax=plt.gca())\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # Group by day of week only\\n\",\n",
    "    \"        dow_spending = daily_features.groupby('dayofweek')['amount_sum'].mean()\\n\",\n",
    "    \"        dow_spending.plot(kind='bar', ax=plt.gca())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.title('Average Spending by Day of Week')\\n\",\n",
    "    \"    plt.xlabel('Day of Week')\\n\",\n",
    "    \"    plt.ylabel('Average Amount')\\n\",\n",
    "    \"    plt.xticks(range(7), day_names, rotation=45)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Day of week feature not available\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Create and Analyze Lagged Features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Add lag features\\n\",\n",
    "    \"daily_with_lags = processor.create_lagged_features(\\n\",\n",
    "    \"    daily_ts,\\n\",\n",
    "    \"    value_col='amount_sum',\\n\",\n",
    "    \"    lag_periods=[1, 7, 30],\\n\",\n",
    "    \"    group_col='category' if 'category' in daily_ts.columns else None\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display lag features\\n\",\n",
    "    \"lag_cols = [col for col in daily_with_lags.columns if '_lag_' in col]\\n\",\n",
    "    \"print(f\\\"Created {len(lag_cols)} lag features: {lag_cols}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Analyze correlation between current spending and lagged spending\\n\",\n",
    "    \"if lag_cols:\\n\",\n",
    "    \"    # Remove rows with NaN values (from lagging)\\n\",\n",
    "    \"    correlation_df = daily_with_lags.dropna(subset=['amount_sum'] + lag_cols)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate correlations\\n\",\n",
    "    \"    correlations = correlation_df[['amount_sum'] + lag_cols].corr()['amount_sum']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot correlations\\n\",\n",
    "    \"    plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"    correlations[1:].sort_values().plot(kind='bar')\\n\",\n",
    "    \"    plt.title('Correlation between Current and Lagged Spending')\\n\",\n",
    "    \"    plt.xlabel('Lag Feature')\\n\",\n",
    "    \"    plt.ylabel('Correlation')\\n\",\n",
    "    \"    plt.axhline(y=0, color='r', linestyle='-')\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"Correlations:\\\")\\n\",\n",
    "    \"    print(correlations)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Create and Analyze Rolling Features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Add rolling features\\n\",\n",
    "    \"daily_with_rolling = processor.create_rolling_features(\\n\",\n",
    "    \"    daily_ts,\\n\",\n",
    "    \"    value_col='amount_sum',\\n\",\n",
    "    \"    window_sizes=[7, 14, 30],\\n\",\n",
    "    \"    group_col='category' if 'category' in daily_ts.columns else None\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display rolling features\\n\",\n",
    "    \"rolling_cols = [col for col in daily_with_rolling.columns if '_rolling_' in col]\\n\",\n",
    "    \"print(f\\\"Created {len(rolling_cols)} rolling features: {rolling_cols}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot original vs rolling mean\\n\",\n",
    "    \"if 'amount_sum_rolling_mean_7' in daily_with_rolling.columns:\\n\",\n",
    "    \"    plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Sort by date for proper time series display\\n\",\n",
    "    \"    plot_df = daily_with_rolling.sort_values('transaction_date')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot for a specific category if available\\n\",\n",
    "    \"    if 'category' in plot_df.columns:\\n\",\n",
    "    \"        # Select a category for demonstration\\n\",\n",
    "    \"        example_category = plot_df['category'].value_counts().index[0]\\n\",\n",
    "    \"        plot_df = plot_df[plot_df['category'] == example_category]\\n\",\n",
    "    \"        title_suffix = f\\\" (Category: {example_category})\\\"\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        title_suffix = \\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot original data and rolling means\\n\",\n",
    "    \"    plt.plot(plot_df['transaction_date'], plot_df['amount_sum'], 'o-', alpha=0.5, label='Daily')\\n\",\n",
    "    \"    plt.plot(plot_df['transaction_date'], plot_df['amount_sum_rolling_mean_7'], 'r-', label='7-day Rolling Mean')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if 'amount_sum_rolling_mean_30' in plot_df.columns:\\n\",\n",
    "    \"        plt.plot(plot_df['transaction_date'], plot_df['amount_sum_rolling_mean_30'], 'g-', label='30-day Rolling Mean')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.title(f'Daily vs. Rolling Average Spending{title_suffix}')\\n\",\n",
    "    \"    plt.xlabel('Date')\\n\",\n",
    "    \"    plt.ylabel('Amount')\\n\",\n",
    "    \"    plt.legend()\\n\",\n",
    "    \"    plt.xticks(rotation=45)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Detect and Analyze Outliers\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Detect outliers\\n\",\n",
    "    \"daily_with_outliers = processor.detect_outliers(\\n\",\n",
    "    \"    daily_ts,\\n\",\n",
    "    \"    value_col='amount_sum',\\n\",\n",
    "    \"    method='zscore',\\n\",\n",
    "    \"    threshold=2.5,  # Lower threshold to identify more outliers for demonstration\\n\",\n",
    "    \"    group_col='category' if 'category' in daily_ts.columns else None\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Count outliers\\n\",\n",
    "    \"outlier_col = 'amount_sum_is_outlier'\\n\",\n",
    "    \"if outlier_col in daily_with_outliers.columns:\\n\",\n",
    "    \"    num_outliers = daily_with_outliers[outlier_col].sum()\\n\",\n",
    "    \"    total_days = len(daily_with_outliers)\\n\",\n",
    "    \"    print(f\\\"Detected {num_outliers} outliers out of {total_days} days ({num_outliers/total_days:.1%})\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # View outliers\\n\",\n",
    "    \"    outliers = daily_with_outliers[daily_with_outliers[outlier_col] == 1]\\n\",\n",
    "    \"    print(\\\"\\\\nTop 5 outliers by amount:\\\")\\n\",\n",
    "    \"    display(outliers.sort_values('amount_sum', ascending=False).head(5)[['transaction_date', 'category', 'amount_sum']])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot outliers\\n\",\n",
    "    \"    plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Sort by date\\n\",\n",
    "    \"    plot_df = daily_with_outliers.sort_values('transaction_date')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot normal points\\n\",\n",
    "    \"    normal = plot_df[plot_df[outlier_col] == 0]\\n\",\n",
    "    \"    plt.scatter(normal['transaction_date'], normal['amount_sum'], color='blue', alpha=0.5, label='Normal')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot outliers\\n\",\n",
    "    \"    outliers = plot_df[plot_df[outlier_col] == 1]\\n\",\n",
    "    \"    plt.scatter(outliers['transaction_date'], outliers['amount_sum'], color='red', alpha=0.8, label='Outlier')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.title('Daily Spending with Outliers Highlighted')\\n\",\n",
    "    \"    plt.xlabel('Date')\\n\",\n",
    "    \"    plt.ylabel('Amount')\\n\",\n",
    "    \"    plt.legend()\\n\",\n",
    "    \"    plt.xticks(rotation=45)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 10. Comprehensive Spending Pattern Visualization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Generate comprehensive spending pattern visualizations\\n\",\n",
    "    \"figures = processor.visualize_spending_patterns(\\n\",\n",
    "    \"    df,\\n\",\n",
    "    \"    date_col='transaction_date',\\n\",\n",
    "    \"    amount_col='amount',\\n\",\n",
    "    \"    category_col='category' if 'category' in df.columns else None,\\n\",\n",
    "    \"    freq='M'  # Monthly aggregation\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Created {len(figures)} visualizations\\\")\\n\",\n",
    "    \"print(f\\\"Visualization files saved to: {processor.visualization_dir}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 11. Save Processed Time Series Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Process full time series data with all features\\n\",\n",
    "    \"processed_daily = processor.process_time_series_data(\\n\",\n",
    "    \"    df,\\n\",\n",
    "    \"    date_col='transaction_date',\\n\",\n",
    "    \"    amount_col='amount',\\n\",\n",
    "    \"    category_col='category' if 'category' in df.columns else None,\\n\",\n",
    "    \"    freq='D'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"processed_weekly = processor.process_time_series_data(\\n\",\n",
    "    \"    df,\\n\",\n",
    "    \"    date_col='transaction_date',\\n\",\n",
    "    \"    amount_col='amount',\\n\",\n",
    "    \"    category_col='category' if 'category' in df.columns else None,\\n\",\n",
    "    \"    freq='W'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"processed_monthly = processor.process_time_series_data(\\n\",\n",
    "    \"    df,\\n\",\n",
    "    \"    date_col='transaction_date',\\n\",\n",
    "    \"    amount_col='amount',\\n\",\n",
    "    \"    category_col='category' if 'category' in df.columns else None,\\n\",\n",
    "    \"    freq='M'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create directory to save processed data\\n\",\n",
    "    \"os.makedirs('../app/data/processed/time_series', exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save processed data\\n\",\n",
    "    \"processed_daily.to_csv('../app/data/processed/time_series/daily_ts.csv', index=False)\\n\",\n",
    "    \"processed_weekly.to_csv('../app/data/processed/time_series/weekly_ts.csv', index=False)\\n\",\n",
    "    \"processed_monthly.to_csv('../app/data/processed/time_series/monthly_ts.csv', index=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Saved processed time series data to app/data/processed/time_series/\\\")\\n\",\n",
    "    \"print(f\\\"Daily time series has {len(processed_daily)} records with {len(processed_daily.columns)} features\\\")\\n\",\n",
    "    \"print(f\\\"Weekly time series has {len(processed_weekly)} records with {len(processed_weekly.columns)} features\\\")\\n\",\n",
    "    \"print(f\\\"Monthly time series has {len(processed_monthly)} records with {len(processed_monthly.columns)} features\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 12. Summary and Next Steps\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this notebook, we've processed financial transaction data into time series format with various temporal features:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Aggregated transactions to daily, weekly, and monthly frequencies\\n\",\n",
    "    \"2. Extracted temporal features (day of week, month, seasonality, etc.)\\n\",\n",
    "    \"3. Created lagged features to capture historical patterns\\n\",\n",
    "    \"4. Created rolling features to smooth out fluctuations\\n\",\n",
    "    \"5. Detected outliers in spending patterns\\n\",\n",
    "    \"6. Visualized spending patterns across different dimensions\\n\",\n",
    "    \"\\n\",\n",
    "    \"These processed time series datasets will be used in the next steps for:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Training forecasting models to predict future spending\\n\",\n",
    "    \"- Developing recommendation engine for personalized financial advice\\n\",\n",
    "    \"- Creating interactive visualizations for the web dashboard\\n\",\n",
    "    \"\\n\",\n",
    "    \"The time series features extracted here will help in identifying spending patterns, seasonal trends, and anomalies, which are crucial for accurate forecasting and meaningful recommendations.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bfbdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
